voxel_size: [0.2, 0.2, 8]
point_cloud_range: [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
#voxel_size: [0.1, 0.1, 0.2]
#point_cloud_range: [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
_dim_: 256
_pos_dim_: _dim_//2
_ffn_dim_: _dim_*2
_num_levels_: 4
bev_h_: 200
bev_w_: 200
queue_length: 4 # each sequence contains `queue_length` frames.

model:
  type: BEVFusion_bevformer
  encoders:
    camera:
      backbone:
        type: SwinTransformer
        embed_dims: 96
        depths: [ 2, 2, 6, 2 ]
        num_heads: [ 3, 6, 12, 24 ]
        window_size: 7
        mlp_ratio: 4
        qkv_bias: true
        qk_scale: null
        drop_rate: 0.
        attn_drop_rate: 0.
        drop_path_rate: 0.2
        patch_norm: true
        out_indices: [ 1, 2, 3 ]
        with_cp: false
        convert_weights: true
        init_cfg:
          type: Pretrained
          checkpoint: /CV/users/zhaohaimei3/bevfusion-main/pretrained/swint-nuimages-pretrained.pth
      neck:
        type: GeneralizedLSSFPN
        in_channels: [ 192, 384, 768 ]
        out_channels: 256
        start_level: 0
        num_outs: 3
        norm_cfg:
          type: BN2d
          requires_grad: true
        act_cfg:
          type: ReLU
          inplace: true
        upsample_cfg:
          mode: bilinear
          align_corners: false
      vtransform:
        in_channels: 256
        out_channels: 80
        feature_size: ${[image_size[0] // 8, image_size[1] // 8]}
        xbound: [ -54.0, 54.0, 0.3 ] #[-51.2, 51.2, 0.4]
        ybound: [ -54.0, 54.0, 0.3 ] #[-51.2, 51.2, 0.4]
        zbound: [ -10.0, 10.0, 20.0 ]
        dbound: [ 1.0, 60.0, 0.5 ]
        downsample: 2
      student_backbone:
        type: ResNet
        depth: 101
        num_stages: 4
        out_indices: [1, 2, 3]
        frozen_stages: 1
        norm_cfg:
          type: BN2d
          requires_grad: False
        norm_eval: True
        style: caffe
        dcn:
          type: DCNv2
          deform_groups: 1
          fallback_on_stride: False # original DCNv2 will print log when perform load_state_dict
        stage_with_dcn: [False, False, True, True]

      student_neck:
        type: FPN
        in_channels: [512, 1024, 2048]
        out_channels: ${_dim_}
        start_level: 0
        add_extra_convs: 'on_output'
        num_outs: 4
        relu_before_extra_convs: true
      student_pts_bbox_head:
        type: BEVFormerHead
        bev_h: ${bev_h_}
        bev_w: ${bev_w_}
        num_query: 900
        num_classes: 10
        in_channels: ${_dim_}
        sync_cls_avg_factor: True
        with_box_refine: True
        as_two_stage: False
        transformer:
          type: PerceptionTransformer
          rotate_prev_bev: True
          use_shift: True
          use_can_bus: True
          embed_dims: ${_dim_}
          encoder:
            type: BEVFormerEncoder
            num_layers: 6
            pc_range: point_cloud_range
            num_points_in_pillar: 4
            return_intermediate: False
            transformerlayers:
              type: BEVFormerLayer
              attn_cfgs:
                -
                  type: TemporalSelfAttention
                  embed_dims: ${_dim_}
                  num_levels: 1
                -
                  type: SpatialCrossAttention
                  pc_range: point_cloud_range
                  deformable_attention:
                    type: MSDeformableAttention3D
                    embed_dims: ${_dim_}
                    num_points: 8
                    num_levels: ${_num_levels_}
                  embed_dims: ${_dim_}
              feedforward_channels: ${_ffn_dim_}
              ffn_dropout: 0.1
              operation_order: ['self_attn', 'norm', 'cross_attn', 'norm', 'ffn', 'norm']
          decoder:
            type: DetectionTransformerDecoder
            num_layers: 6
            return_intermediate: True
            transformerlayers:
              type: DetrTransformerDecoderLayer
              attn_cfgs:
                -
                  type: MultiheadAttention
                  embed_dims: ${_dim_}
                  num_heads: 8
                  dropout: 0.1
                -
                  type: CustomMSDeformableAttention
                  embed_dims: ${_dim_}
                  num_levels: 1
              feedforward_channels: ${_ffn_dim_}
              ffn_dropout: 0.1
              operation_order: ['self_attn', 'norm', 'cross_attn', 'norm', 'ffn', 'norm']
        bbox_coder:
          type: 'NMSFreeCoder'
          post_center_range: ${[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]}
          pc_range: ${point_cloud_range}
          max_num: 300
          voxel_size: voxel_size
          num_classes: 10
        positional_encoding:
          type: 'LearnedPositionalEncoding'
          num_feats: 128
          row_num_embed: 200
          col_num_embed: 200
    lidar:
      voxelize:
        point_cloud_range: ${point_cloud_range}
        voxel_size: ${voxel_size}
        max_voxels: [ 120000, 160000 ]
      backbone:
        sparse_shape: [ 1440, 1440, 41 ]
  decoder:
    backbone:
      type: GeneralizedResNet
      in_channels: 256
      blocks:
        - [2, 128, 2]
        - [2, 256, 2]
        - [2, 512, 1]
    neck:
      type: LSSFPN
      in_indices: [-1, 0]
      in_channels: [512, 128]
      out_channels: 256
      scale_factor: 2

optimizer:
  paramwise_cfg:
    custom_keys:
      absolute_pos_embed:
        decay_mult: 0
      relative_position_bias_table:
        decay_mult: 0
      encoders.camera.backbone:
        lr_mult: 0.1


lr_config:
  policy: cyclic
  target_ratio: 5.0
  cyclic_times: 1
  step_ratio_up: 0.4

momentum_config:
  policy: cyclic
  cyclic_times: 1
  step_ratio_up: 0.4

data:
  samples_per_gpu: 6
